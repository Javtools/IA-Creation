{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Website URL Crawler in Google Colab\n",
    "This notebook allows you to crawl internal URLs from a website and download them as an Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 pandas xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_website_urls(base_url):\n",
    "    visited = set()\n",
    "    to_visit = set([base_url])\n",
    "    unique_urls = set()\n",
    "    domain = urlparse(base_url).netloc\n",
    "\n",
    "    print(f\"Starting to crawl the website: {base_url}\")\n",
    "\n",
    "    while to_visit:\n",
    "        current_url = to_visit.pop()\n",
    "        if current_url not in visited:\n",
    "            visited.add(current_url)\n",
    "            print(f\"Visiting URL: {current_url}\")\n",
    "            try:\n",
    "                response = requests.get(current_url, timeout=10)\n",
    "                response.encoding = 'utf-8'\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'lxml')\n",
    "                    for link in soup.find_all('a', href=True):\n",
    "                        href = link['href']\n",
    "                        full_url = urljoin(base_url, href)\n",
    "                        if urlparse(full_url).netloc == domain and full_url not in visited:\n",
    "                            to_visit.add(full_url)\n",
    "                            unique_urls.add(full_url)\n",
    "                else:\n",
    "                    print(f\"Error: Received status code {response.status_code} for {current_url}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error crawling {current_url}: {e}\")\n",
    "\n",
    "    print(f\"Finished crawling. Found {len(unique_urls)} unique internal URLs.\")\n",
    "    return sorted(unique_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_urls_hierarchically(urls):\n",
    "    hierarchy = []\n",
    "    sorted_urls = sorted(urls, key=lambda x: x.split('/'))\n",
    "    max_depth = max(len(urlparse(url).path.strip('/').split('/')) for url in sorted_urls if urlparse(url).path.strip('/'))\n",
    "\n",
    "    for url in sorted_urls:\n",
    "        path = urlparse(url).path.strip('/')\n",
    "        if not path:\n",
    "            continue\n",
    "        parts = path.split('/')\n",
    "        levels = [\"/\" + \"/\".join(parts[:i+1]) for i in range(len(parts))]\n",
    "        while len(levels) < max_depth:\n",
    "            levels.append(\"\")\n",
    "        hierarchy.append((*levels, url))\n",
    "\n",
    "    return hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_excel(hierarchy):\n",
    "    max_levels = max(len(entry) - 1 for entry in hierarchy)\n",
    "    columns = [f'Navigation Level {i+1}' for i in range(max_levels)] + ['Current URL Address']\n",
    "    df = pd.DataFrame(hierarchy, columns=columns)\n",
    "\n",
    "    buffer = BytesIO()\n",
    "    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:\n",
    "        df.to_excel(writer, index=False, sheet_name='URLs')\n",
    "    buffer.seek(0)\n",
    "\n",
    "    with open(\"website_urls.xlsx\", \"wb\") as f:\n",
    "        f.write(buffer.read())\n",
    "    \n",
    "    return \"website_urls.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Input URL and run everything\n",
    "base_url = input(\"Enter the base URL (e.g., https://example.com): \").strip()\n",
    "\n",
    "if base_url and base_url.startswith(\"http\"):\n",
    "    urls = get_website_urls(base_url)\n",
    "    if urls:\n",
    "        hierarchy = organize_urls_hierarchically(urls)\n",
    "        print(f\"‚úÖ Found {len(hierarchy)} URLs.\")\n",
    "        file_path = export_to_excel(hierarchy)\n",
    "        print(\"üì• Download the Excel file below:\")\n",
    "        display(FileLink(file_path))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No URLs found.\")\n",
    "else:\n",
    "    print(\"‚ùå Please enter a valid URL starting with 'http' or 'https'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
